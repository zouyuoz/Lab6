{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b10c90",
   "metadata": {},
   "source": [
    "# 2025 DL Lab6: Text Summarization with Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5a614",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm 邱照元, 314834001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1d9c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assignment involves implementing a hybrid sequence-to-sequence model to perform text summarization on the SAMSum and Reddit TIFU datasets.\n",
    "\n",
    "The model architecture is composed of two main parts:\n",
    "A pre-trained model utilized as the encoder.\n",
    "A new decoder which must be implemented from scratch.\n",
    "\n",
    "The objective is to fine-tune the existing encoder while training the custom decoder from the beginning, enabling the complete model to generate accurate and concise summaries. Performance is measured using the standard summarization metric: ROUGE-L Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76750bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu129\n",
      "Python Major.Minor: 3.12\n",
      "transformers version: 4.57.3\n",
      "ABI: True\n",
      "Flash Attention version: 2.8.3\n",
      "Is CUDA available: True\n",
      "CUDA Version: 12.9\n",
      "Device Name: NVIDIA GeForce RTX 5090\n",
      "Flash Attention calculation successful.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import flash_attn\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Python Major.Minor: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"ABI: {torch._C._GLIBCXX_USE_CXX11_ABI}\")\n",
    "print(f\"Flash Attention version: {flash_attn.__version__}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 簡單測試 (若無報錯即成功)\n",
    "q = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "from flash_attn import flash_attn_func\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Flash Attention calculation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bce78d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "from data_utils import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformer.Const import *\n",
    "from transformer.Models import Seq2SeqModelWithFlashAttn\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODE = \"train\"  # set to \"predict\" for inference\n",
    "CHECKPOINT_PATH = Path(\"checkpoints/latest.pt\")\n",
    "BEST_CHECKPOINT_PATH = Path(\"checkpoints/best.pt\")\n",
    "PREDICT_CHECKPOINT = Path(\"checkpoints/best.pt\")\n",
    "TIFU_TEST_PATH = Path(\"dataset/tifu/tifu_test.jsonl\")\n",
    "SAMSUN_TEST_PATH = Path(\"dataset/samsun/test.csv\")\n",
    "PREDICTION_OUTPUT = Path(\"result.csv\")\n",
    "MAX_TARGET_LEN = 512\n",
    "MAX_GENERATION_LEN = MAX_TARGET_LEN\n",
    "TRAIN_EPOCHS = 40\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "GLOBAL_SEED = 42\n",
    "NUM_WORKERS = 4\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d696f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f16bc4",
   "metadata": {},
   "source": [
    "## CREATE DATASET\n",
    "use ConCate dataset to handle multiple datasets situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68ce8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    path: List[Optional[str]],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    require_target: bool = True,\n",
    ") -> Optional[Dataset]:\n",
    "    if all(p is None for p in path):\n",
    "        return None\n",
    "    datasets = []\n",
    "    for p in path:\n",
    "        if p is not None:\n",
    "            dataset = SquadSeq2SeqDataset(\n",
    "                Path(p), tokenizer, max_source_len=MAX_SOURCE_LEN, max_target_len=MAX_TARGET_LEN, require_target=require_target\n",
    "            )\n",
    "            datasets.append(dataset)\n",
    "    print(f\"Built dataset with {sum(len(ds) for ds in datasets)} samples.\")\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n",
    "\n",
    "def build_dataloader(\n",
    "    source: Union[Optional[Dataset], Optional[str]],\n",
    "    batch_size: int = 4,\n",
    "    shuffle: bool = False,\n",
    "    num_workers: int = 8,\n",
    ") -> Optional[DataLoader]:\n",
    "    dataset = source\n",
    "    collator = QACollator # Don't forget to define QACollator in data_utils.py\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collator,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b937d6",
   "metadata": {},
   "source": [
    "## Main loop of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1b1f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    device: torch.device,\n",
    "    optimizer: Optional[torch.optim.Optimizer],\n",
    "    scheduler: Optional[object],\n",
    "    pad_id: int,\n",
    "    max_grad_norm: float,\n",
    "    train: bool,\n",
    "    accumulation_steps: int = 1,  # [新增] 梯度累積步數，預設 1 代表不累積\n",
    ") -> float:\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    iterator = tqdm(dataloader, desc=\"train\" if train else \"eval\", leave=False)\n",
    "    \n",
    "    # 確保訓練開始前梯度是清空的\n",
    "    if train and optimizer is not None:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # 注意：這裡的 src 和 tgt 都是 1D 的 Packed Tensor\n",
    "        src = batch[\"src\"].to(device) # (Total_Src_Tokens,)\n",
    "        tgt = batch[\"tgt\"].to(device) # (Total_Tgt_Tokens,)\n",
    "        src_len = batch[\"src_len\"].to(device) # (B,)\n",
    "        tgt_len = batch[\"tgt_len\"].to(device) # (B,)\n",
    "\n",
    "        ############### YOUR CODE HERE (FLASH ATTENTION VERSION) ###############\n",
    "        \n",
    "        # 1. 計算累積長度來找出每個句子的邊界\n",
    "        cu_len = torch.cumsum(tgt_len, dim=0, dtype=torch.long)\n",
    "        \n",
    "        # 找出每個句子最後一個 Token (EOS) 的位置 -> 用於 Decoder Input (要移除)\n",
    "        end_indices = cu_len - 1\n",
    "        \n",
    "        # 找出每個句子第一個 Token (BOS) 的位置 -> 用於 Labels (要移除)\n",
    "        start_indices = cu_len - tgt_len\n",
    "        \n",
    "        # 2. 製作 Mask 並切片\n",
    "        total_tokens = tgt.size(0)\n",
    "        \n",
    "        # 製作 Decoder Input: 保留所有 token，但移除每個句子的最後一個 token (EOS)\n",
    "        mask_in = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_in[end_indices] = False\n",
    "        dec_in = tgt[mask_in]\n",
    "        \n",
    "        # 製作 Labels: 保留所有 token，但移除每個句子的第一個 token (BOS)\n",
    "        mask_label = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_label[start_indices] = False\n",
    "        labels = tgt[mask_label]\n",
    "        \n",
    "        # 調整長度 (每個句子都少了一個 token)\n",
    "        dec_len = tgt_len - 1\n",
    "\n",
    "        # 3. Forward Pass\n",
    "        logits = model(\n",
    "            src_input_ids=src,\n",
    "            trg_input_ids=dec_in,\n",
    "            src_seq_len=src_len,\n",
    "            trg_seq_len=dec_len\n",
    "        )\n",
    "\n",
    "        # 4. Compute Loss\n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=pad_id)\n",
    "        \n",
    "        ######################################################\n",
    "        \n",
    "        if train:\n",
    "            # [修改 1] 將 Loss 除以累積步數\n",
    "            # 因為梯度是累加的，Loss 也要平均，否則梯度會變成原本的 accumulation_steps 倍大\n",
    "            loss = loss / accumulation_steps\n",
    "            \n",
    "            # [修改 2] 反向傳播 (這時候梯度會累積在 .grad 屬性中，不會被清空)\n",
    "            loss.backward()\n",
    "            \n",
    "            # [修改 3] 判斷是否達到累積步數，才進行參數更新\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                # 梯度裁剪 (Gradient Clipping)\n",
    "                clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                \n",
    "                # 更新參數 (Optimizer Step)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 更新學習率 (Scheduler Step)\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                \n",
    "                # 清空梯度 (Zero Grad)\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        # 紀錄 Loss (記得乘回來以便觀察原始數值)\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        steps += 1\n",
    "        iterator.set_postfix(loss=total_loss / max(1, steps))\n",
    "        \n",
    "    return total_loss / max(1, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c1705",
   "metadata": {},
   "source": [
    "## Checkpoints management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f457f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    path: Path,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[object],\n",
    "    path: Path,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    if scheduler is not None and hasattr(scheduler, \"state_dict\"):\n",
    "        state[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "    torch.save(state, path)\n",
    "\n",
    "# (選用) 如果你要繼續訓練，建議改用這個版本來同時載入 Optimizer\n",
    "def load_checkpoint_for_training(\n",
    "    model, optimizer, scheduler, path, device\n",
    "):\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and \"scheduler_state_dict\" in state:\n",
    "        scheduler.load_state_dict(state[\"scheduler_state_dict\"])\n",
    "    print(f\"已載入 Epoch {state['epoch']} 的完整訓練狀態\")\n",
    "    return state[\"epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079efad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd5cccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters and arguments ###\n",
    "lr = 2e-4\n",
    "weight_decay = 0.001\n",
    "epochs = TRAIN_EPOCHS\n",
    "max_grad_norm = 1.0\n",
    "batch_size = TRAIN_BATCH_SIZE\n",
    "num_workers = NUM_WORKERS\n",
    "#####################################\n",
    "set_seed(GLOBAL_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA is required to run this code.\")\n",
    "\n",
    "# Check if flash attention is available\n",
    "try:\n",
    "    import flash_attn  # noqa: F401\n",
    "except ImportError:\n",
    "    raise ImportError(\"flash_attn is required to run this code.\")\n",
    "\n",
    "model = Seq2SeqModelWithFlashAttn(\n",
    "    transformer_model_path=\"answerdotai/ModernBERT-base\",\n",
    "    freeze_encoder=True,\n",
    ").to(device)\n",
    "tokenizer = model.tokenizer\n",
    "checkpoint_path = CHECKPOINT_PATH\n",
    "best_checkpoint_path = BEST_CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset with 44229 samples.\n",
      "Built dataset with 5030 samples.\n"
     ]
    }
   ],
   "source": [
    "train_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_train.jsonl\", \"dataset/samsun/train.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "train_loader = build_dataloader(\n",
    "    train_set,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "val_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_val.jsonl\", \"dataset/samsun/validation.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "valid_loader = build_dataloader(\n",
    "    val_set,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "total_steps = epochs * len(train_loader)\n",
    "warmup_steps = total_steps // 10\n",
    "print(warmup_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=min(warmup_steps, total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529b628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 確保你已經定義了 model 和 device，並引入了 Path\n",
    "# from pathlib import Path\n",
    "# latest_ckpt_path = Path(\"checkpoints/latest.pt\")\n",
    "# if latest_ckpt_path.exists():\n",
    "#     start_epoch = load_checkpoint_for_training(model, optimizer, scheduler, latest_ckpt_path, device)\n",
    "#     print(f\"成功載入模型權重：{latest_ckpt_path}\")\n",
    "# else:\n",
    "#     print(f\"找不到檔案：{latest_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9c08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - train loss: 7.9569 | val loss: 6.1877 | ppl: 486.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 - train loss: 5.9567 | val loss: 5.5502 | ppl: 257.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 - train loss: 5.4354 | val loss: 5.2110 | ppl: 183.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 - train loss: 4.9740 | val loss: 4.8310 | ppl: 125.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 - train loss: 4.5471 | val loss: 4.5833 | ppl: 97.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 - train loss: 4.2197 | val loss: 4.3984 | ppl: 81.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 - train loss: 3.9558 | val loss: 4.2682 | ppl: 71.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 - train loss: 3.7231 | val loss: 4.2099 | ppl: 67.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 - train loss: 3.5112 | val loss: 4.1602 | ppl: 64.08\n",
      "\n",
      "[Info] Unfreezing Encoder at epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 - train loss: 3.2905 | val loss: 3.9894 | ppl: 54.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 - train loss: 3.0450 | val loss: 3.9836 | ppl: 53.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 - train loss: 2.8170 | val loss: 4.0209 | ppl: 55.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 - train loss: 2.5767 | val loss: 4.0769 | ppl: 58.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 - train loss: 2.3344 | val loss: 4.1476 | ppl: 63.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 - train loss: 2.0884 | val loss: 4.2539 | ppl: 70.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 - train loss: 1.8465 | val loss: 4.3479 | ppl: 77.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 - train loss: 1.6219 | val loss: 4.4654 | ppl: 86.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 - train loss: 1.4183 | val loss: 4.5866 | ppl: 98.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 - train loss: 1.2344 | val loss: 4.7157 | ppl: 111.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 - train loss: 1.0696 | val loss: 4.8267 | ppl: 124.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 - train loss: 0.9213 | val loss: 4.9262 | ppl: 137.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 - train loss: 0.8013 | val loss: 5.0451 | ppl: 155.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 - train loss: 0.6977 | val loss: 5.1593 | ppl: 174.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 - train loss: 0.6111 | val loss: 5.2716 | ppl: 194.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 - train loss: 0.5409 | val loss: 5.3659 | ppl: 213.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 - train loss: 0.4805 | val loss: 5.4237 | ppl: 226.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 - train loss: 0.4312 | val loss: 5.5098 | ppl: 247.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 - train loss: 0.3901 | val loss: 5.5693 | ppl: 262.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 - train loss: 0.3553 | val loss: 5.6036 | ppl: 271.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 - train loss: 0.3263 | val loss: 5.6902 | ppl: 295.94\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m tqdm.write(msg)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:    \n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     76\u001b[39m     current_val_ppl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m current_val_ppl < best_val_ppl\n\u001b[32m     78\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m best_checkpoint_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     79\u001b[39m ):\n\u001b[32m     80\u001b[39m     best_val_ppl = current_val_ppl\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36msave_checkpoint\u001b[39m\u001b[34m(model, optimizer, scheduler, path, epoch)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(scheduler, \u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     23\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mscheduler_state_dict\u001b[39m\u001b[33m\"\u001b[39m] = scheduler.state_dict()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/.venv/lib/python3.12/site-packages/torch/serialization.py:967\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/.venv/lib/python3.12/site-packages/torch/serialization.py:1268\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1266\u001b[39m         storage = storage.cpu()\n\u001b[32m   1267\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. 初始化記錄用的 list\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_ppls = []\n",
    "UNFREEZE_EPOCH = 10\n",
    "ACCUMULATION_STEPS = 4\n",
    "\n",
    "best_val_ppl = float(\"inf\")\n",
    "\n",
    "for epoch in range(start_epoch + 1, epochs + 1):\n",
    "\n",
    "    # --- [新增] 動態解凍 Encoder ---\n",
    "    if epoch == UNFREEZE_EPOCH:\n",
    "        print(f\"\\n[Info] Unfreezing Encoder at epoch {epoch}...\")\n",
    "        # 1. 將 Encoder 的參數設定為需要梯度\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # 2. (選用) 如果您希望解凍後 Encoder 使用較小的 Learning Rate，\n",
    "        #    可以在這裡重建 optimizer，或者直接繼續使用原本的 (會繼承目前的 LR)\n",
    "        #    最簡單的做法是甚麼都不用做，Optimizer 會自動開始更新這些變成 requires_grad=True 的參數\n",
    "    # -----------------------------\n",
    "\n",
    "    train_loss = run_epoch(\n",
    "        train_loader,\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        tokenizer.pad_token_id,\n",
    "        max_grad_norm,\n",
    "        train=True,\n",
    "        accumulation_steps=ACCUMULATION_STEPS  # 傳入這個參數\n",
    "    )\n",
    "    \n",
    "    # 2. 記錄 Training Loss\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    msg = f\"Epoch {epoch}/{epochs} - train loss: {train_loss:.4f}\"\n",
    "    current_val_ppl = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = run_epoch(\n",
    "            valid_loader,\n",
    "            model,\n",
    "            device,\n",
    "            optimizer=None,\n",
    "            scheduler=None,\n",
    "            pad_id=tokenizer.pad_token_id,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            train=False,\n",
    "        )\n",
    "    \n",
    "    perplexity = math.exp(min(20, val_loss))\n",
    "    current_val_ppl = perplexity\n",
    "    \n",
    "    # 3. 記錄 Validation Loss 和 Perplexity\n",
    "    val_losses.append(val_loss)\n",
    "    val_ppls.append(perplexity)\n",
    "\n",
    "    msg += f\" | val loss: {val_loss:.4f} | ppl: {perplexity:.2f}\"\n",
    "    tqdm.write(msg)\n",
    "    \n",
    "    if checkpoint_path is not None:    \n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    if (\n",
    "        current_val_ppl is not None\n",
    "        and current_val_ppl < best_val_ppl\n",
    "        and best_checkpoint_path is not None\n",
    "    ):\n",
    "        best_val_ppl = current_val_ppl\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=best_checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbfbd183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] Training log successfully saved to _log/log_11302241.csv\n"
     ]
    }
   ],
   "source": [
    "# 4. 訓練結束後，將記錄寫入 CSV\n",
    "timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
    "log_filename = f\"_log/log_{timestamp}.csv\"\n",
    "\n",
    "try:\n",
    "    with open(log_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # 寫入標頭\n",
    "        writer.writerow([\"train_loss\", \"val_loss\", \"ppl\"])\n",
    "        # 寫入數據\n",
    "        for t, v, p in zip(train_losses, val_losses, val_ppls):\n",
    "            writer.writerow([t, v, p])\n",
    "    print(f\"\\n[Info] Training log successfully saved to {log_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[Error] Failed to save training log: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acffb21",
   "metadata": {},
   "source": [
    "## Predict Result\n",
    "\n",
    "Predict the labesl based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. To kaggle. Click \"Submit Predictions\"\n",
    "2. Upload the result.csv\n",
    "3. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0baffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset with 9248 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "load_checkpoint(model, PREDICT_CHECKPOINT, device)\n",
    "model.eval()\n",
    "test_set = build_dataset(\n",
    "    [TIFU_TEST_PATH, SAMSUN_TEST_PATH],\n",
    "    tokenizer=model.tokenizer,\n",
    "    require_target=False,\n",
    ")\n",
    "test_loader = build_dataloader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "predictions: List[Tuple[str, str]] = []\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(test_loader, desc=\"predict\", leave=False):\n",
    "        input_ids = sample[\"src\"].to(device)\n",
    "        src_lens = sample[\"src_len\"].to(device=device, dtype=torch.int32)\n",
    "        ids = sample[\"id\"] #list of ids\n",
    "        summaries = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            src_seq_len=src_lens,\n",
    "            generation_limit=MAX_GENERATION_LEN,\n",
    "            sampling=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predictions.extend(zip(ids, summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d9e218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 9248 predictions to _result/result_11302241.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
    "output_path = Path(f\"_result/result_{timestamp}.csv\")\n",
    "write_predictions_csv(output_path, predictions)\n",
    "print(f\"Wrote {len(predictions)} predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "236bb36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_EPOCHS = 40\n",
      "Inintial freeze encode, unfreeze at 10\n",
      "Best happened at epoch = 11\n",
      "TRAIN_BATCH_SIZE = 32\n",
      "But using gradient accumulation = 4\n",
      "WARMUP_STEP = 1024\n",
      "LERANING_RATE = 0.0002\n",
      "MAX_SOURCE_LEN = 1270\n",
      "d_inner = 768 * 4\n",
      "\n",
      "100%|███████████████████████████████████████| 1.02M/1.02M [00:01<00:00, 786kB/s]\n",
      "Successfully submitted to Lab6 Text Summarization with Seq2Seq Model(639401)"
     ]
    }
   ],
   "source": [
    "\n",
    "Message = (\"\"\n",
    "    +f\"TRAIN_EPOCHS = {TRAIN_EPOCHS}\\n\"\n",
    "    +f\"Inintial freeze encode, unfreeze at {UNFREEZE_EPOCH}\\n\"\n",
    "    +f\"Best happened at epoch = 11\\n\"\n",
    "    +f\"TRAIN_BATCH_SIZE = {TRAIN_BATCH_SIZE}\\n\"\n",
    "    +f\"But using gradient accumulation = {ACCUMULATION_STEPS}\\n\"\n",
    "    +f\"WARMUP_STEP = {warmup_steps}\\n\"\n",
    "    +f\"LERANING_RATE = {lr}\\n\"\n",
    "    +f\"MAX_SOURCE_LEN = {MAX_SOURCE_LEN}\\n\"\n",
    "    +\"d_inner = 768 * 4\\n\"\n",
    ")\n",
    "!echo \"{Message}\"\n",
    "!kaggle competitions submit -c lab-6-training-a-seq-2-seq-model-on-s-qu-ad-639401 -f {output_path} -m \"{Message}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476bb26",
   "metadata": {},
   "source": [
    "解決方案\n",
    "\n",
    "如果您希望使用較大的 Batch Size (如 128 或 256) 並且維持效能，建議您：\n",
    "\n",
    "1. 增加 Epochs：把 Epochs 增加到 40 或 50，補回失去的 Steps。\n",
    "1. 調大 Learning Rate：嘗試將 LR 調回 1e-4 甚至 2e-4。\n",
    "1. 減少 Warmup：將 warmup_steps 設為總步數的 5%~10% (例如 BS=128 時設為 500~1000)。\n",
    "\n",
    "如果要在現有架構上改進，CP 值最高的順序是：\n",
    "\n",
    "1. Decoder 改 Pre-Norm：改幾行程式碼而已，結構更穩。\n",
    "1. 調整 Dimension：把 d_inner 改回 768 * 4。\n",
    "1. Decoder 先 Train：這其實就是您一開始 freeze_encoder=True 的狀態！您可以先跑 5 個 Epoch (Freeze)，然後再跑 25 個 Epoch (Unfreeze)。\n",
    "1. LoRA：這需要引入 peft 套件，改動稍大，但如果是為了比賽衝分，這是必殺技。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
