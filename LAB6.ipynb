{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b10c90",
   "metadata": {},
   "source": [
    "# 2025 DL Lab6: Text Summarization with Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5a614",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm 邱照元, 314834001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1d9c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assignment involves implementing a hybrid sequence-to-sequence model to perform text summarization on the SAMSum and Reddit TIFU datasets.\n",
    "\n",
    "The model architecture is composed of two main parts:\n",
    "A pre-trained model utilized as the encoder.\n",
    "A new decoder which must be implemented from scratch.\n",
    "\n",
    "The objective is to fine-tune the existing encoder while training the custom decoder from the beginning, enabling the complete model to generate accurate and concise summaries. Performance is measured using the standard summarization metric: ROUGE-L Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd788a",
   "metadata": {},
   "source": [
    "## Kaggle Competition\n",
    "Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish datasets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n",
    "\n",
    "This assignment use kaggle to calculate your grade.  \n",
    "Please use this [**LINK**](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b) to join the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcb906",
   "metadata": {},
   "source": [
    "## Unzip Data\n",
    "\n",
    "Unzip dataset.zip\n",
    "\n",
    "### SAMSum\n",
    "+ `train` : 14700\n",
    "+ `val` : 818\n",
    "+ `test` : 819\n",
    "\n",
    "### Redit_TIFU\n",
    "+ `train` : 29498\n",
    "+ `val` : 4212\n",
    "+ `test` : 8429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76750bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Python Major.Minor: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"ABI: {torch._C._GLIBCXX_USE_CXX11_ABI}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import flash_attn\n",
    "print(f\"Flash Attention version: {flash_attn.__version__}\")\n",
    "# 簡單測試 (若無報錯即成功)\n",
    "q = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "from flash_attn import flash_attn_func\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Flash Attention calculation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce78d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "from data_utils import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformer.Const import *\n",
    "from transformer.Models import Seq2SeqModelWithFlashAttn\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODE = \"train\"  # set to \"predict\" for inference\n",
    "CHECKPOINT_PATH = Path(\"checkpoints/latest.pt\")\n",
    "BEST_CHECKPOINT_PATH = Path(\"checkpoints/best.pt\")\n",
    "PREDICT_CHECKPOINT = Path(\"checkpoints/best.pt\")\n",
    "TIFU_TEST_PATH = Path(\"dataset/tifu/tifu_test.jsonl\")\n",
    "SAMSUN_TEST_PATH = Path(\"dataset/samsun/test.csv\")\n",
    "PREDICTION_OUTPUT = Path(\"result.csv\")\n",
    "MAX_TARGET_LEN = 512\n",
    "MAX_GENERATION_LEN = MAX_TARGET_LEN\n",
    "TRAIN_EPOCHS = 30\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "GLOBAL_SEED = 42\n",
    "NUM_WORKERS = 4\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f16bc4",
   "metadata": {},
   "source": [
    "## CREATE DATASET\n",
    "use ConCate dataset to handle multiple datasets situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    path: List[Optional[str]],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    require_target: bool = True,\n",
    ") -> Optional[Dataset]:\n",
    "    if all(p is None for p in path):\n",
    "        return None\n",
    "    datasets = []\n",
    "    for p in path:\n",
    "        if p is not None:\n",
    "            dataset = SquadSeq2SeqDataset(\n",
    "                Path(p), tokenizer, max_source_len=MAX_SOURCE_LEN, max_target_len=MAX_TARGET_LEN, require_target=require_target\n",
    "            )\n",
    "            datasets.append(dataset)\n",
    "    print(f\"Built dataset with {sum(len(ds) for ds in datasets)} samples.\")\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n",
    "\n",
    "def build_dataloader(\n",
    "    source: Union[Optional[Dataset], Optional[str]],\n",
    "    batch_size: int = 4,\n",
    "    shuffle: bool = False,\n",
    "    num_workers: int = 8,\n",
    ") -> Optional[DataLoader]:\n",
    "    dataset = source\n",
    "    collator = QACollator # Don't forget to define QACollator in data_utils.py\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collator,\n",
    "        num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b937d6",
   "metadata": {},
   "source": [
    "## Main loop of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    device: torch.device,\n",
    "    optimizer: Optional[torch.optim.Optimizer],\n",
    "    scheduler: Optional[object],\n",
    "    pad_id: int,\n",
    "    max_grad_norm: float,\n",
    "    train: bool,\n",
    ") -> float:\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    iterator = tqdm(dataloader, desc=\"train\" if train else \"eval\", leave=False)\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # 注意：這裡的 src 和 tgt 都是 1D 的 Packed Tensor\n",
    "        src = batch[\"src\"].to(device) # (Total_Src_Tokens,)\n",
    "        tgt = batch[\"tgt\"].to(device) # (Total_Tgt_Tokens,)\n",
    "        src_len = batch[\"src_len\"].to(device) # (B,)\n",
    "        tgt_len = batch[\"tgt_len\"].to(device) # (B,)\n",
    "\n",
    "        ############### YOUR CODE HERE (FLASH ATTENTION VERSION) ###############\n",
    "        \n",
    "        # 因為 tgt 是多個句子串接在一起的 (Packed)，我們不能直接用 [:, :-1] 切片\n",
    "        # 我們需要利用 cumsum 找出每個句子的邊界\n",
    "        \n",
    "        # 1. 計算累積長度來找出每個句子的邊界\n",
    "        cu_len = torch.cumsum(tgt_len, dim=0, dtype=torch.long)\n",
    "        \n",
    "        # 找出每個句子最後一個 Token (EOS) 的位置 -> 用於 Decoder Input (要移除)\n",
    "        end_indices = cu_len - 1\n",
    "        \n",
    "        # 找出每個句子第一個 Token (BOS) 的位置 -> 用於 Labels (要移除)\n",
    "        start_indices = cu_len - tgt_len\n",
    "        \n",
    "        # 2. 製作 Mask 並切片\n",
    "        total_tokens = tgt.size(0)\n",
    "        \n",
    "        # 製作 Decoder Input: 保留所有 token，但移除每個句子的最後一個 token (EOS)\n",
    "        mask_in = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_in[end_indices] = False\n",
    "        dec_in = tgt[mask_in]\n",
    "        \n",
    "        # 製作 Labels: 保留所有 token，但移除每個句子的第一個 token (BOS)\n",
    "        mask_label = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_label[start_indices] = False\n",
    "        labels = tgt[mask_label]\n",
    "        \n",
    "        # 調整長度 (每個句子都少了一個 token)\n",
    "        dec_len = tgt_len - 1\n",
    "\n",
    "        # 3. Forward Pass\n",
    "        logits = model(\n",
    "            src_input_ids=src,\n",
    "            trg_input_ids=dec_in,\n",
    "            src_seq_len=src_len,\n",
    "            trg_seq_len=dec_len\n",
    "        )\n",
    "\n",
    "        # 4. Compute Loss\n",
    "        # logits: (Total_Tokens, Vocab_Size), labels: (Total_Tokens,)\n",
    "        # 直接計算 CrossEntropy，不需要再 reshape\n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=pad_id)\n",
    "        \n",
    "        ######################################################\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        iterator.set_postfix(loss=total_loss / max(1, steps))\n",
    "        \n",
    "    return total_loss / max(1, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c1705",
   "metadata": {},
   "source": [
    "## Checkpoints management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f457f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    path: Path,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[object],\n",
    "    path: Path,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    if scheduler is not None and hasattr(scheduler, \"state_dict\"):\n",
    "        state[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "    torch.save(state, path)\n",
    "\n",
    "# (選用) 如果你要繼續訓練，建議改用這個版本來同時載入 Optimizer\n",
    "def load_checkpoint_for_training(\n",
    "    model, optimizer, scheduler, path, device\n",
    "):\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and \"scheduler_state_dict\" in state:\n",
    "        scheduler.load_state_dict(state[\"scheduler_state_dict\"])\n",
    "    print(f\"已載入 Epoch {state['epoch']} 的完整訓練狀態\")\n",
    "    return state[\"epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079efad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters and arguments ###\n",
    "lr = 8e-5\n",
    "weight_decay = 0.001\n",
    "warmup_steps = 2000\n",
    "epochs = TRAIN_EPOCHS\n",
    "max_grad_norm = 1.0\n",
    "batch_size = TRAIN_BATCH_SIZE\n",
    "num_workers = NUM_WORKERS\n",
    "#####################################\n",
    "set_seed(GLOBAL_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA is required to run this code.\")\n",
    "\n",
    "# Check if flash attention is available\n",
    "try:\n",
    "    import flash_attn  # noqa: F401\n",
    "except ImportError:\n",
    "    raise ImportError(\"flash_attn is required to run this code.\")\n",
    "\n",
    "model = Seq2SeqModelWithFlashAttn(\n",
    "    transformer_model_path=\"answerdotai/ModernBERT-base\",\n",
    "    freeze_encoder=True,\n",
    ").to(device)\n",
    "tokenizer = model.tokenizer\n",
    "checkpoint_path = CHECKPOINT_PATH\n",
    "best_checkpoint_path = BEST_CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_train.jsonl\", \"dataset/samsun/train.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "train_loader = build_dataloader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "val_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_val.jsonl\", \"dataset/samsun/validation.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "valid_loader = build_dataloader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "total_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=min(warmup_steps, total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 確保你已經定義了 model 和 device，並引入了 Path\n",
    "# from pathlib import Path\n",
    "# latest_ckpt_path = Path(\"checkpoints/latest.pt\")\n",
    "# if latest_ckpt_path.exists():\n",
    "#     start_epoch = load_checkpoint_for_training(model, optimizer, scheduler, latest_ckpt_path, device)\n",
    "#     print(f\"成功載入模型權重：{latest_ckpt_path}\")\n",
    "# else:\n",
    "#     print(f\"找不到檔案：{latest_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. 初始化記錄用的 list\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_ppls = []\n",
    "\n",
    "best_val_ppl = float(\"inf\")\n",
    "\n",
    "for epoch in range(start_epoch + 1, epochs + 1):\n",
    "    train_loss = run_epoch(\n",
    "        train_loader,\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        tokenizer.pad_token_id,\n",
    "        max_grad_norm,\n",
    "        train=True,\n",
    "    )\n",
    "    \n",
    "    # 2. 記錄 Training Loss\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    msg = f\"Epoch {epoch}/{epochs} - train loss: {train_loss:.4f}\"\n",
    "    current_val_ppl = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = run_epoch(\n",
    "            valid_loader,\n",
    "            model,\n",
    "            device,\n",
    "            optimizer=None,\n",
    "            scheduler=None,\n",
    "            pad_id=tokenizer.pad_token_id,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            train=False,\n",
    "        )\n",
    "    \n",
    "    perplexity = math.exp(min(20, val_loss))\n",
    "    current_val_ppl = perplexity\n",
    "    \n",
    "    # 3. 記錄 Validation Loss 和 Perplexity\n",
    "    val_losses.append(val_loss)\n",
    "    val_ppls.append(perplexity)\n",
    "\n",
    "    msg += f\" | val loss: {val_loss:.4f} | ppl: {perplexity:.2f}\"\n",
    "    print(msg)\n",
    "    \n",
    "    if checkpoint_path is not None:    \n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    if (\n",
    "        current_val_ppl is not None\n",
    "        and current_val_ppl < best_val_ppl\n",
    "        and best_checkpoint_path is not None\n",
    "    ):\n",
    "        best_val_ppl = current_val_ppl\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=best_checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "# 4. 訓練結束後，將記錄寫入 CSV\n",
    "# 生成帶有時間戳的檔名，例如: training_log_20241027_153000.csv\n",
    "timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
    "log_filename = f\"log_{timestamp}.csv\"\n",
    "\n",
    "try:\n",
    "    with open(log_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # 寫入標頭\n",
    "        writer.writerow([\"train_loss\", \"val_loss\", \"ppl\"])\n",
    "        # 寫入數據\n",
    "        for t, v, p in zip(train_losses, val_losses, val_ppls):\n",
    "            writer.writerow([t, v, p])\n",
    "    print(f\"\\n[Info] Training log successfully saved to {log_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[Error] Failed to save training log: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acffb21",
   "metadata": {},
   "source": [
    "## Predict Result\n",
    "\n",
    "Predict the labesl based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. To kaggle. Click \"Submit Predictions\"\n",
    "2. Upload the result.csv\n",
    "3. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, PREDICT_CHECKPOINT, device)\n",
    "model.eval()\n",
    "test_set = build_dataset(\n",
    "    [TIFU_TEST_PATH, SAMSUN_TEST_PATH],\n",
    "    tokenizer=model.tokenizer,\n",
    "    require_target=False,\n",
    ")\n",
    "test_loader = build_dataloader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "predictions: List[Tuple[str, str]] = []\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(test_loader, desc=\"predict\", leave=False):\n",
    "        input_ids = sample[\"src\"].to(device)\n",
    "        src_lens = sample[\"src_len\"].to(device=device, dtype=torch.int32)\n",
    "        ids = sample[\"id\"] #list of ids\n",
    "        summaries = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            src_seq_len=src_lens,\n",
    "            generation_limit=MAX_GENERATION_LEN,\n",
    "            sampling=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predictions.extend(zip(ids, summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(f\"result_{datetime.now().strftime(\"%m%d%H%M\")}.csv\")\n",
    "write_predictions_csv(output_path, predictions)\n",
    "print(f\"Wrote {len(predictions)} predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Message = (\"\"\n",
    "    +f\"TRAIN_EPOCHS = {TRAIN_EPOCHS}\\n\"\n",
    "    +f\"TRAIN_BATCH_SIZE = {TRAIN_BATCH_SIZE}\\n\"\n",
    "    +f\"LERANING_RATE = {lr}\\n\"\n",
    "    +f\"MAX_SOURCE_LEN = {MAX_SOURCE_LEN}\\n\"\n",
    "    # +\"citerion1=nn.CrossEntropyLoss()\\n\"\n",
    ")\n",
    "!echo \"{Message}\"\n",
    "!kaggle competitions submit -c lab-6-training-a-seq-2-seq-model-on-s-qu-ad-639401 -f {output_path} -m \"{Message}\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7dcde",
   "metadata": {},
   "source": [
    "11282304:\n",
    "TRAIN_EPOCHS = 30\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "LERANING_RATE = 8e-05\n",
    "MAX_SOURCE_LEN = 2048\n",
    "\n",
    "11282345:\n",
    "TRAIN_EPOCHS = 30\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "LERANING_RATE = 8e-05\n",
    "MAX_SOURCE_LEN = 1024\n",
    "\n",
    "11290034:\n",
    "TRAIN_EPOCHS = 30\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "LERANING_RATE = 8e-05\n",
    "MAX_SOURCE_LEN = 512"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
