{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b10c90",
   "metadata": {},
   "source": [
    "# 2025 DL Lab6: Text Summarization with Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c25439",
   "metadata": {},
   "source": [
    "Before we start, please put **your name** and **SID** in following format: <br>\n",
    "Hi I'm 陸仁賈, 314831000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5a614",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm 邱照元, 314834001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1d9c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assignment involves implementing a hybrid sequence-to-sequence model to perform text summarization on the SAMSum and Reddit TIFU datasets.\n",
    "\n",
    "The model architecture is composed of two main parts:\n",
    "A pre-trained model utilized as the encoder.\n",
    "A new decoder which must be implemented from scratch.\n",
    "\n",
    "The objective is to fine-tune the existing encoder while training the custom decoder from the beginning, enabling the complete model to generate accurate and concise summaries. Performance is measured using the standard summarization metric: ROUGE-L Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd788a",
   "metadata": {},
   "source": [
    "## Kaggle Competition\n",
    "Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish datasets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n",
    "\n",
    "This assignment use kaggle to calculate your grade.  \n",
    "Please use this [**LINK**](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b) to join the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcb906",
   "metadata": {},
   "source": [
    "## Unzip Data\n",
    "\n",
    "Unzip dataset.zip\n",
    "\n",
    "### SAMSum\n",
    "+ `train` : 14700\n",
    "+ `val` : 818\n",
    "+ `test` : 819\n",
    "\n",
    "### Redit_TIFU\n",
    "+ `train` : 29498\n",
    "+ `val` : 4212\n",
    "+ `test` : 8429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04484b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nmixx\n",
    "\"\"\"\n",
    "pytorch\n",
    "numpy\n",
    "kaggle\n",
    "datasets\n",
    "pyarrow\n",
    "transformer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a9ca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu128\n",
      "CUDA Version: 12.8\n",
      "Python Major.Minor: 3.12\n",
      "ABI: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Python Major.Minor: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"ABI: {torch._C._GLIBCXX_USE_CXX11_ABI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce78d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "from data_utils import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformer.Constants import *\n",
    "from transformer.Models import Seq2SeqModelWithFlashAttn\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODE = \"train\"  # set to \"predict\" for inference\n",
    "CHECKPOINT_PATH = Path(\"checkpoints/latest.pt\")\n",
    "BEST_CHECKPOINT_PATH = Path(\"checkpoints/best.pt\")\n",
    "PREDICT_CHECKPOINT = Path(\"checkpoints/best.pt\")\n",
    "TIFU_TEST_PATH = Path(\"dataset/tifu/tifu_test.jsonl\")\n",
    "SAMSUN_TEST_PATH = Path(\"dataset/samsun/test.csv\")\n",
    "PREDICTION_OUTPUT = Path(\"result.csv\")\n",
    "MAX_TARGET_LEN = 512\n",
    "MAX_GENERATION_LEN = MAX_TARGET_LEN\n",
    "TRAIN_EPOCHS = 30\n",
    "TRAIN_BATCH_SIZE = 100\n",
    "GLOBAL_SEED = 42\n",
    "NUM_WORKERS = 4\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f16bc4",
   "metadata": {},
   "source": [
    "## CREATE DATASET\n",
    "use ConCate dataset to handle multiple datasets situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    path: List[Optional[str]],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    require_target: bool = True,\n",
    ") -> Optional[Dataset]:\n",
    "    if all(p is None for p in path):\n",
    "        return None\n",
    "    datasets = []\n",
    "    for p in path:\n",
    "        if p is not None:\n",
    "            dataset = SquadSeq2SeqDataset(\n",
    "                Path(p), tokenizer, max_source_len=MAX_SOURCE_LEN, max_target_len=MAX_TARGET_LEN, require_target=require_target\n",
    "            )\n",
    "            datasets.append(dataset)\n",
    "    print(f\"Built dataset with {sum(len(ds) for ds in datasets)} samples.\")\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n",
    "\n",
    "def build_dataloader(\n",
    "    source: Union[Optional[Dataset], Optional[str]],\n",
    "    batch_size: int = 4,\n",
    "    shuffle: bool = False,\n",
    "    num_workers: int = 8,\n",
    ") -> Optional[DataLoader]:\n",
    "    dataset = source\n",
    "    collator = QACollator # Don't forget to define QACollator in data_utils.py\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collator,\n",
    "        num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b937d6",
   "metadata": {},
   "source": [
    "## Main loop of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    device: torch.device,\n",
    "    optimizer: Optional[torch.optim.Optimizer],\n",
    "    scheduler: Optional[object],\n",
    "    pad_id: int,\n",
    "    max_grad_norm: float,\n",
    "    train: bool,\n",
    ") -> float:\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    iterator = tqdm(dataloader, desc=\"train\" if train else \"eval\", leave=False)\n",
    "    \n",
    "    for batch in iterator:\n",
    "        src = batch[\"src\"].to(device) # (B, L_src)\n",
    "        tgt = batch[\"tgt\"].to(device) # (B, L_tgt), content: [BOS, w1, ..., EOS, PAD...]\n",
    "\n",
    "        # 檢查 tgt 是否合法\n",
    "        if tgt.size(1) < 2:\n",
    "             raise ValueError(\"Each target sequence must contain at least BOS and EOS tokens.\")\n",
    "\n",
    "        ############### YOUR CODE HERE (FIXED) ###############\n",
    "        \n",
    "        # 1. 準備 Decoder Input 和 Labels\n",
    "        # Decoder Input: 去掉最後一個 token (通常是 EOS 或 PAD)，保留 BOS 開頭\n",
    "        dec_in = tgt[:, :-1] \n",
    "        # Labels: 去掉第一個 token (BOS)，保留 EOS 結尾以計算 Loss\n",
    "        labels = tgt[:, 1:]\n",
    "\n",
    "        # 2. 生成 Masks (因為改回了標準 Attention，需要自行生成 Mask)\n",
    "        # Source Mask: 標記 PAD 的位置為 False (或依模型實作而定，這裡假設 True 為保留)\n",
    "        # Models.py 的 get_pad_mask 邏輯是 (seq != pad_idx).unsqueeze(-2)\n",
    "        src_mask = (src != pad_id).unsqueeze(-2).to(device)\n",
    "\n",
    "        # Target Mask: 結合 Padding Mask 與 Sequence Mask (Causal Mask)\n",
    "        trg_pad_mask = (dec_in != pad_id).unsqueeze(-2).to(device)\n",
    "        \n",
    "        sz_b, len_s = dec_in.size()\n",
    "        # 建立下三角矩陣 (Causal Mask)\n",
    "        subsequent_mask = torch.tril(torch.ones((len_s, len_s), device=device)).bool()\n",
    "        trg_mask = trg_pad_mask & subsequent_mask.unsqueeze(0)\n",
    "\n",
    "        # 3. Forward Pass\n",
    "        # 注意：這裡改為傳入 mask，而非 seq_len\n",
    "        logits = model(\n",
    "            src_input_ids=src,\n",
    "            trg_input_ids=dec_in,\n",
    "            src_mask=src_mask,\n",
    "            trg_mask=trg_mask\n",
    "        )\n",
    "\n",
    "        # 4. Compute Loss\n",
    "        # 將 logits 展平為 (B * (L-1), Vocab_Size) 以計算 CrossEntropy\n",
    "        # ignore_index=pad_id 會自動忽略 Padding 的 Loss\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1), ignore_index=pad_id)\n",
    "        \n",
    "        ######################################################\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        iterator.set_postfix(loss=total_loss / max(1, steps))\n",
    "        \n",
    "    return total_loss / max(1, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c1705",
   "metadata": {},
   "source": [
    "## Checkpoints management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f457f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    path: Path,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[object],\n",
    "    path: Path,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    if scheduler is not None and hasattr(scheduler, \"state_dict\"):\n",
    "        state[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "    torch.save(state, path)\n",
    "\n",
    "# (選用) 如果你要繼續訓練，建議改用這個版本來同時載入 Optimizer\n",
    "def load_checkpoint_for_training(\n",
    "    model, optimizer, scheduler, path, device\n",
    "):\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and \"scheduler_state_dict\" in state:\n",
    "        scheduler.load_state_dict(state[\"scheduler_state_dict\"])\n",
    "    print(f\"已載入 Epoch {state['epoch']} 的完整訓練狀態\")\n",
    "    return state[\"epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079efad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters and arguments ###\n",
    "lr = 1e-4\n",
    "weight_decay = 0.001\n",
    "warmup_steps = 2000\n",
    "epochs = TRAIN_EPOCHS\n",
    "max_grad_norm = 1.0\n",
    "batch_size = TRAIN_BATCH_SIZE\n",
    "num_workers = NUM_WORKERS\n",
    "#####################################\n",
    "set_seed(GLOBAL_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA is required to run this code.\")\n",
    "\n",
    "# Check if flash attention is available\n",
    "# try:\n",
    "#     import flash_attn  # noqa: F401\n",
    "# except ImportError:\n",
    "#     raise ImportError(\"flash_attn is required to run this code.\")\n",
    "\n",
    "model = Seq2SeqModelWithFlashAttn(\n",
    "    transformer_model_path=\"answerdotai/ModernBERT-base\",\n",
    "    freeze_encoder=True,\n",
    ").to(device)\n",
    "tokenizer = model.tokenizer\n",
    "checkpoint_path = CHECKPOINT_PATH\n",
    "best_checkpoint_path = BEST_CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_train.jsonl\", \"dataset/samsun/train.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "train_loader = build_dataloader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "val_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_val.jsonl\", \"dataset/samsun/validation.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "valid_loader = build_dataloader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "total_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=min(warmup_steps, total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 確保你已經定義了 model 和 device，並引入了 Path\n",
    "# from pathlib import Path\n",
    "# latest_ckpt_path = Path(\"checkpoints/latest.pt\")\n",
    "# start_epoch = 0\n",
    "# if latest_ckpt_path.exists():\n",
    "#     start_epoch = load_checkpoint_for_training(model, optimizer, scheduler, latest_ckpt_path, device)\n",
    "#     print(f\"成功載入模型權重：{latest_ckpt_path}\")\n",
    "# else:\n",
    "#     print(f\"找不到檔案：{latest_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_ppl = float(\"inf\")\n",
    "for epoch in range(start_epoch + 1, epochs + 1):\n",
    "    train_loss = run_epoch(\n",
    "        train_loader,\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        tokenizer.pad_token_id,\n",
    "        max_grad_norm,\n",
    "        train=True,\n",
    "    )\n",
    "    msg = f\"Epoch {epoch}/{epochs} - train loss: {train_loss:.4f}\"\n",
    "    current_val_ppl = None\n",
    "    with torch.no_grad():\n",
    "        val_loss = run_epoch(\n",
    "            valid_loader,\n",
    "            model,\n",
    "            device,\n",
    "            optimizer=None,\n",
    "            scheduler=None,\n",
    "            pad_id=tokenizer.pad_token_id,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            train=False,\n",
    "        )\n",
    "    perplexity = math.exp(min(20, val_loss))\n",
    "    current_val_ppl = perplexity\n",
    "    msg += f\" | val loss: {val_loss:.4f} | ppl: {perplexity:.2f}\"\n",
    "    print(msg)\n",
    "    if checkpoint_path is not None:    \n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    if (\n",
    "        current_val_ppl is not None\n",
    "        and current_val_ppl < best_val_ppl\n",
    "        and best_checkpoint_path is not None\n",
    "    ):\n",
    "        best_val_ppl = current_val_ppl\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=best_checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acffb21",
   "metadata": {},
   "source": [
    "## Predict Result\n",
    "\n",
    "Predict the labesl based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. To kaggle. Click \"Submit Predictions\"\n",
    "2. Upload the result.csv\n",
    "3. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, PREDICT_CHECKPOINT, device)\n",
    "model.eval()\n",
    "test_set = build_dataset(\n",
    "    [TIFU_TEST_PATH, SAMSUN_TEST_PATH],\n",
    "    tokenizer=model.tokenizer,\n",
    "    require_target=False,\n",
    ")\n",
    "test_loader = build_dataloader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "predictions: List[Tuple[str, str]] = []\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(test_loader, desc=\"predict\", leave=False):\n",
    "        input_ids = sample[\"src\"].to(device)\n",
    "        src_lens = sample[\"src_len\"].to(device=device, dtype=torch.int32)\n",
    "        ids = sample[\"id\"] #list of ids\n",
    "        summaries = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            src_seq_len=src_lens,\n",
    "            generation_limit=MAX_GENERATION_LEN,\n",
    "            sampling=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predictions.extend(zip(ids, summaries))\n",
    "output_path = PREDICTION_OUTPUT\n",
    "write_predictions_csv(output_path, predictions)\n",
    "print(f\"Wrote {len(predictions)} predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c lab-6-training-a-seq-2-seq-model-on-s-qu-ad-639401 -f result.csv -m \"Message\"    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
