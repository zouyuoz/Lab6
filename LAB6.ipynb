{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b10c90",
   "metadata": {},
   "source": [
    "# 2025 DL Lab6: Text Summarization with Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5a614",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm 邱照元, 314834001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1d9c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assignment involves implementing a hybrid sequence-to-sequence model to perform text summarization on the SAMSum and Reddit TIFU datasets.\n",
    "\n",
    "The model architecture is composed of two main parts:\n",
    "A pre-trained model utilized as the encoder.\n",
    "A new decoder which must be implemented from scratch.\n",
    "\n",
    "The objective is to fine-tune the existing encoder while training the custom decoder from the beginning, enabling the complete model to generate accurate and concise summaries. Performance is measured using the standard summarization metric: ROUGE-L Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76750bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Lab6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu129\n",
      "Python Major.Minor: 3.12\n",
      "transformers version: 4.57.3\n",
      "ABI: True\n",
      "Flash Attention version: 2.8.3\n",
      "Is CUDA available: True\n",
      "CUDA Version: 12.9\n",
      "Device Name: NVIDIA GeForce RTX 5090\n",
      "Flash Attention calculation successful.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import flash_attn\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Python Major.Minor: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"ABI: {torch._C._GLIBCXX_USE_CXX11_ABI}\")\n",
    "print(f\"Flash Attention version: {flash_attn.__version__}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 簡單測試 (若無報錯即成功)\n",
    "q = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "from flash_attn import flash_attn_func\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Flash Attention calculation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce78d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "from data_utils import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformer.Const import *\n",
    "from transformer.Models import Seq2SeqModelWithFlashAttn\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODE = \"train\"  # set to \"predict\" for inference\n",
    "CHECKPOINT_PATH = Path(\"checkpoints/latest.pt\")\n",
    "BEST_CHECKPOINT_PATH = Path(\"checkpoints/best.pt\")\n",
    "PREDICT_CHECKPOINT = Path(\"checkpoints/best.pt\")\n",
    "TIFU_TEST_PATH = Path(\"dataset/tifu/tifu_test.jsonl\")\n",
    "SAMSUN_TEST_PATH = Path(\"dataset/samsun/test.csv\")\n",
    "PREDICTION_OUTPUT = Path(\"result.csv\")\n",
    "MAX_TARGET_LEN = 512\n",
    "MAX_GENERATION_LEN = MAX_TARGET_LEN\n",
    "TRAIN_EPOCHS = 40\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "GLOBAL_SEED = 42\n",
    "NUM_WORKERS = 4\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d696f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f16bc4",
   "metadata": {},
   "source": [
    "## CREATE DATASET\n",
    "use ConCate dataset to handle multiple datasets situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ce8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    path: List[Optional[str]],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    require_target: bool = True,\n",
    ") -> Optional[Dataset]:\n",
    "    if all(p is None for p in path):\n",
    "        return None\n",
    "    datasets = []\n",
    "    for p in path:\n",
    "        if p is not None:\n",
    "            dataset = SquadSeq2SeqDataset(\n",
    "                Path(p), tokenizer, max_source_len=MAX_SOURCE_LEN, max_target_len=MAX_TARGET_LEN, require_target=require_target\n",
    "            )\n",
    "            datasets.append(dataset)\n",
    "    print(f\"Built dataset with {sum(len(ds) for ds in datasets)} samples.\")\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n",
    "\n",
    "def build_dataloader(\n",
    "    source: Union[Optional[Dataset], Optional[str]],\n",
    "    batch_size: int = 4,\n",
    "    shuffle: bool = False,\n",
    "    num_workers: int = 8,\n",
    ") -> Optional[DataLoader]:\n",
    "    dataset = source\n",
    "    collator = QACollator # Don't forget to define QACollator in data_utils.py\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collator,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b937d6",
   "metadata": {},
   "source": [
    "## Main loop of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b1f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    device: torch.device,\n",
    "    optimizer: Optional[torch.optim.Optimizer],\n",
    "    scheduler: Optional[object],\n",
    "    pad_id: int,\n",
    "    max_grad_norm: float,\n",
    "    train: bool,\n",
    ") -> float:\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    iterator = tqdm(dataloader, desc=\"train\" if train else \"eval\", leave=False)\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # 注意：這裡的 src 和 tgt 都是 1D 的 Packed Tensor\n",
    "        src = batch[\"src\"].to(device) # (Total_Src_Tokens,)\n",
    "        tgt = batch[\"tgt\"].to(device) # (Total_Tgt_Tokens,)\n",
    "        src_len = batch[\"src_len\"].to(device) # (B,)\n",
    "        tgt_len = batch[\"tgt_len\"].to(device) # (B,)\n",
    "\n",
    "        ############### YOUR CODE HERE (FLASH ATTENTION VERSION) ###############\n",
    "        \n",
    "        # 因為 tgt 是多個句子串接在一起的 (Packed)，我們不能直接用 [:, :-1] 切片\n",
    "        # 我們需要利用 cumsum 找出每個句子的邊界\n",
    "        \n",
    "        # 1. 計算累積長度來找出每個句子的邊界\n",
    "        cu_len = torch.cumsum(tgt_len, dim=0, dtype=torch.long)\n",
    "        \n",
    "        # 找出每個句子最後一個 Token (EOS) 的位置 -> 用於 Decoder Input (要移除)\n",
    "        end_indices = cu_len - 1\n",
    "        \n",
    "        # 找出每個句子第一個 Token (BOS) 的位置 -> 用於 Labels (要移除)\n",
    "        start_indices = cu_len - tgt_len\n",
    "        \n",
    "        # 2. 製作 Mask 並切片\n",
    "        total_tokens = tgt.size(0)\n",
    "        \n",
    "        # 製作 Decoder Input: 保留所有 token，但移除每個句子的最後一個 token (EOS)\n",
    "        mask_in = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_in[end_indices] = False\n",
    "        dec_in = tgt[mask_in]\n",
    "        \n",
    "        # 製作 Labels: 保留所有 token，但移除每個句子的第一個 token (BOS)\n",
    "        mask_label = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_label[start_indices] = False\n",
    "        labels = tgt[mask_label]\n",
    "        \n",
    "        # 調整長度 (每個句子都少了一個 token)\n",
    "        dec_len = tgt_len - 1\n",
    "\n",
    "        # 3. Forward Pass\n",
    "        logits = model(\n",
    "            src_input_ids=src,\n",
    "            trg_input_ids=dec_in,\n",
    "            src_seq_len=src_len,\n",
    "            trg_seq_len=dec_len\n",
    "        )\n",
    "\n",
    "        # 4. Compute Loss\n",
    "        # logits: (Total_Tokens, Vocab_Size), labels: (Total_Tokens,)\n",
    "        # 直接計算 CrossEntropy，不需要再 reshape\n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=pad_id)\n",
    "        \n",
    "        ######################################################\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        iterator.set_postfix(loss=total_loss / max(1, steps))\n",
    "        \n",
    "    return total_loss / max(1, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c1705",
   "metadata": {},
   "source": [
    "## Checkpoints management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f457f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    path: Path,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[object],\n",
    "    path: Path,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    if scheduler is not None and hasattr(scheduler, \"state_dict\"):\n",
    "        state[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "    torch.save(state, path)\n",
    "\n",
    "# (選用) 如果你要繼續訓練，建議改用這個版本來同時載入 Optimizer\n",
    "def load_checkpoint_for_training(\n",
    "    model, optimizer, scheduler, path, device\n",
    "):\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and \"scheduler_state_dict\" in state:\n",
    "        scheduler.load_state_dict(state[\"scheduler_state_dict\"])\n",
    "    print(f\"已載入 Epoch {state['epoch']} 的完整訓練狀態\")\n",
    "    return state[\"epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079efad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5cccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters and arguments ###\n",
    "lr = 2e-4\n",
    "weight_decay = 0.001\n",
    "warmup_steps = 1024\n",
    "epochs = TRAIN_EPOCHS\n",
    "max_grad_norm = 1.0\n",
    "batch_size = TRAIN_BATCH_SIZE\n",
    "num_workers = NUM_WORKERS\n",
    "#####################################\n",
    "set_seed(GLOBAL_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA is required to run this code.\")\n",
    "\n",
    "# Check if flash attention is available\n",
    "try:\n",
    "    import flash_attn  # noqa: F401\n",
    "except ImportError:\n",
    "    raise ImportError(\"flash_attn is required to run this code.\")\n",
    "\n",
    "model = Seq2SeqModelWithFlashAttn(\n",
    "    transformer_model_path=\"answerdotai/ModernBERT-base\",\n",
    "    freeze_encoder=False,\n",
    ").to(device)\n",
    "tokenizer = model.tokenizer\n",
    "checkpoint_path = CHECKPOINT_PATH\n",
    "best_checkpoint_path = BEST_CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c39fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset with 44229 samples.\n",
      "Built dataset with 5030 samples.\n"
     ]
    }
   ],
   "source": [
    "train_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_train.jsonl\", \"dataset/samsun/train.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "train_loader = build_dataloader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "val_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_val.jsonl\", \"dataset/samsun/validation.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "valid_loader = build_dataloader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "total_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=min(warmup_steps, total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529b628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 確保你已經定義了 model 和 device，並引入了 Path\n",
    "# from pathlib import Path\n",
    "# latest_ckpt_path = Path(\"checkpoints/latest.pt\")\n",
    "# if latest_ckpt_path.exists():\n",
    "#     start_epoch = load_checkpoint_for_training(model, optimizer, scheduler, latest_ckpt_path, device)\n",
    "#     print(f\"成功載入模型權重：{latest_ckpt_path}\")\n",
    "# else:\n",
    "#     print(f\"找不到檔案：{latest_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9c08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     19\u001b[39m         param.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# 2. (選用) 如果您希望解凍後 Encoder 使用較小的 Learning Rate，\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m#    可以在這裡重建 optimizer，或者直接繼續使用原本的 (會繼承目前的 LR)\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m#    最簡單的做法是甚麼都不用做，Optimizer 會自動開始更新這些變成 requires_grad=True 的參數\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m train_loss = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 2. 記錄 Training Loss\u001b[39;00m\n\u001b[32m     38\u001b[39m train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(dataloader, model, device, optimizer, scheduler, pad_id, max_grad_norm, train)\u001b[39m\n\u001b[32m     51\u001b[39m dec_len = tgt_len - \u001b[32m1\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 3. Forward Pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrg_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdec_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrg_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdec_len\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# 4. Compute Loss\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# logits: (Total_Tokens, Vocab_Size), labels: (Total_Tokens,)\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 直接計算 CrossEntropy，不需要再 reshape\u001b[39;00m\n\u001b[32m     64\u001b[39m loss = F.cross_entropy(logits, labels, ignore_index=pad_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/transformer/Models.py:179\u001b[39m, in \u001b[36mSeq2SeqModelWithFlashAttn.forward\u001b[39m\u001b[34m(self, src_input_ids, trg_input_ids, src_seq_len, trg_seq_len)\u001b[39m\n\u001b[32m    176\u001b[39m enc_output_packed = last_hidden_state[attention_mask.bool()]\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# 4. Decoder Forward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m dec_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m(\n\u001b[32m    180\u001b[39m     trg_seq=trg_input_ids,\n\u001b[32m    181\u001b[39m     trg_mask=trg_seq_len,\n\u001b[32m    182\u001b[39m     enc_output=enc_output_packed,\n\u001b[32m    183\u001b[39m     src_mask=src_seq_len\n\u001b[32m    184\u001b[39m )\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Project to vocabulary\u001b[39;00m\n\u001b[32m    187\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.output_projection(dec_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1949\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1944\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1950\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1951\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. 初始化記錄用的 list\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_ppls = []\n",
    "UNFREEZE_EPOCH = 10\n",
    "\n",
    "best_val_ppl = float(\"inf\")\n",
    "\n",
    "for epoch in range(start_epoch + 1, epochs + 1):\n",
    "\n",
    "    # --- [新增] 動態解凍 Encoder ---\n",
    "    if epoch == UNFREEZE_EPOCH:\n",
    "        print(f\"\\n[Info] Unfreezing Encoder at epoch {epoch}...\")\n",
    "        # 1. 將 Encoder 的參數設定為需要梯度\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # 2. (選用) 如果您希望解凍後 Encoder 使用較小的 Learning Rate，\n",
    "        #    可以在這裡重建 optimizer，或者直接繼續使用原本的 (會繼承目前的 LR)\n",
    "        #    最簡單的做法是甚麼都不用做，Optimizer 會自動開始更新這些變成 requires_grad=True 的參數\n",
    "    # -----------------------------\n",
    "\n",
    "    train_loss = run_epoch(\n",
    "        train_loader,\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        tokenizer.pad_token_id,\n",
    "        max_grad_norm,\n",
    "        train=True,\n",
    "    )\n",
    "    \n",
    "    # 2. 記錄 Training Loss\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    msg = f\"Epoch {epoch}/{epochs} - train loss: {train_loss:.4f}\"\n",
    "    current_val_ppl = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = run_epoch(\n",
    "            valid_loader,\n",
    "            model,\n",
    "            device,\n",
    "            optimizer=None,\n",
    "            scheduler=None,\n",
    "            pad_id=tokenizer.pad_token_id,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            train=False,\n",
    "        )\n",
    "    \n",
    "    perplexity = math.exp(min(20, val_loss))\n",
    "    current_val_ppl = perplexity\n",
    "    \n",
    "    # 3. 記錄 Validation Loss 和 Perplexity\n",
    "    val_losses.append(val_loss)\n",
    "    val_ppls.append(perplexity)\n",
    "\n",
    "    msg += f\" | val loss: {val_loss:.4f} | ppl: {perplexity:.2f}\"\n",
    "    tqdm.write(msg)\n",
    "    \n",
    "    if checkpoint_path is not None:    \n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    if (\n",
    "        current_val_ppl is not None\n",
    "        and current_val_ppl < best_val_ppl\n",
    "        and best_checkpoint_path is not None\n",
    "    ):\n",
    "        best_val_ppl = current_val_ppl\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=best_checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 訓練結束後，將記錄寫入 CSV\n",
    "timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
    "log_filename = f\"_log/log_{timestamp}.csv\"\n",
    "\n",
    "try:\n",
    "    with open(log_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # 寫入標頭\n",
    "        writer.writerow([\"train_loss\", \"val_loss\", \"ppl\"])\n",
    "        # 寫入數據\n",
    "        for t, v, p in zip(train_losses, val_losses, val_ppls):\n",
    "            writer.writerow([t, v, p])\n",
    "    print(f\"\\n[Info] Training log successfully saved to {log_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[Error] Failed to save training log: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acffb21",
   "metadata": {},
   "source": [
    "## Predict Result\n",
    "\n",
    "Predict the labesl based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. To kaggle. Click \"Submit Predictions\"\n",
    "2. Upload the result.csv\n",
    "3. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, PREDICT_CHECKPOINT, device)\n",
    "model.eval()\n",
    "test_set = build_dataset(\n",
    "    [TIFU_TEST_PATH, SAMSUN_TEST_PATH],\n",
    "    tokenizer=model.tokenizer,\n",
    "    require_target=False,\n",
    ")\n",
    "test_loader = build_dataloader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "predictions: List[Tuple[str, str]] = []\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(test_loader, desc=\"predict\", leave=False):\n",
    "        input_ids = sample[\"src\"].to(device)\n",
    "        src_lens = sample[\"src_len\"].to(device=device, dtype=torch.int32)\n",
    "        ids = sample[\"id\"] #list of ids\n",
    "        summaries = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            src_seq_len=src_lens,\n",
    "            generation_limit=MAX_GENERATION_LEN,\n",
    "            sampling=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predictions.extend(zip(ids, summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
    "output_path = Path(f\"_result/result_{timestamp}.csv\")\n",
    "write_predictions_csv(output_path, predictions)\n",
    "print(f\"Wrote {len(predictions)} predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Message = (\"\"\n",
    "    +f\"TRAIN_EPOCHS = {TRAIN_EPOCHS}\\n\"\n",
    "    +f\"TRAIN_BATCH_SIZE = {TRAIN_BATCH_SIZE}\\n\"\n",
    "    +f\"WARMUP_STEP = {warmup_steps}\\n\"\n",
    "    +f\"LERANING_RATE = {lr}\\n\"\n",
    "    +f\"MAX_SOURCE_LEN = {MAX_SOURCE_LEN}\\n\"\n",
    "    +\"d_inner = 768 * 2\\n\"\n",
    ")\n",
    "!echo \"{Message}\"\n",
    "!kaggle competitions submit -c lab-6-training-a-seq-2-seq-model-on-s-qu-ad-639401 -f {output_path} -m \"{Message}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476bb26",
   "metadata": {},
   "source": [
    "解決方案\n",
    "\n",
    "如果您希望使用較大的 Batch Size (如 128 或 256) 並且維持效能，建議您：\n",
    "\n",
    "1. 增加 Epochs：把 Epochs 增加到 40 或 50，補回失去的 Steps。\n",
    "1. 調大 Learning Rate：嘗試將 LR 調回 1e-4 甚至 2e-4。\n",
    "1. 減少 Warmup：將 warmup_steps 設為總步數的 5%~10% (例如 BS=128 時設為 500~1000)。\n",
    "\n",
    "如果要在現有架構上改進，CP 值最高的順序是：\n",
    "\n",
    "1. Decoder 改 Pre-Norm：改幾行程式碼而已，結構更穩。\n",
    "1. 調整 Dimension：把 d_inner 改回 768 * 4。\n",
    "1. Decoder 先 Train：這其實就是您一開始 freeze_encoder=True 的狀態！您可以先跑 5 個 Epoch (Freeze)，然後再跑 25 個 Epoch (Unfreeze)。\n",
    "1. LoRA：這需要引入 peft 套件，改動稍大，但如果是為了比賽衝分，這是必殺技。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
