{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b10c90",
   "metadata": {},
   "source": [
    "# 2025 DL Lab6: Text Summarization with Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5a614",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm 邱照元, 314834001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1d9c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assignment involves implementing a hybrid sequence-to-sequence model to perform text summarization on the SAMSum and Reddit TIFU datasets.\n",
    "\n",
    "The model architecture is composed of two main parts:\n",
    "A pre-trained model utilized as the encoder.\n",
    "A new decoder which must be implemented from scratch.\n",
    "\n",
    "The objective is to fine-tune the existing encoder while training the custom decoder from the beginning, enabling the complete model to generate accurate and concise summaries. Performance is measured using the standard summarization metric: ROUGE-L Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd788a",
   "metadata": {},
   "source": [
    "## Kaggle Competition\n",
    "Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish datasets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n",
    "\n",
    "This assignment use kaggle to calculate your grade.  \n",
    "Please use this [**LINK**](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b) to join the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcb906",
   "metadata": {},
   "source": [
    "## Unzip Data\n",
    "\n",
    "Unzip dataset.zip\n",
    "\n",
    "### SAMSum\n",
    "+ `train` : 14700\n",
    "+ `val` : 818\n",
    "+ `test` : 819\n",
    "\n",
    "### Redit_TIFU\n",
    "+ `train` : 29498\n",
    "+ `val` : 4212\n",
    "+ `test` : 8429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76750bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Lab6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu129\n",
      "Python Major.Minor: 3.12\n",
      "transformers version: 4.57.3\n",
      "ABI: True\n",
      "Is CUDA available: True\n",
      "CUDA Version: 12.9\n",
      "Device Name: NVIDIA GeForce RTX 5090\n",
      "Flash Attention version: 2.8.3\n",
      "Flash Attention calculation successful.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Python Major.Minor: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"ABI: {torch._C._GLIBCXX_USE_CXX11_ABI}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import flash_attn\n",
    "print(f\"Flash Attention version: {flash_attn.__version__}\")\n",
    "# 簡單測試 (若無報錯即成功)\n",
    "q = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "from flash_attn import flash_attn_func\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Flash Attention calculation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce78d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "from data_utils import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformer.Const import *\n",
    "from transformer.Models import Seq2SeqModelWithFlashAttn\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODE = \"train\"  # set to \"predict\" for inference\n",
    "CHECKPOINT_PATH = Path(\"checkpoints/latest.pt\")\n",
    "BEST_CHECKPOINT_PATH = Path(\"checkpoints/best.pt\")\n",
    "PREDICT_CHECKPOINT = Path(\"checkpoints/best.pt\")\n",
    "TIFU_TEST_PATH = Path(\"dataset/tifu/tifu_test.jsonl\")\n",
    "SAMSUN_TEST_PATH = Path(\"dataset/samsun/test.csv\")\n",
    "PREDICTION_OUTPUT = Path(\"result.csv\")\n",
    "MAX_TARGET_LEN = 512\n",
    "MAX_GENERATION_LEN = MAX_TARGET_LEN\n",
    "TRAIN_EPOCHS = 30\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "GLOBAL_SEED = 42\n",
    "NUM_WORKERS = 4\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d696f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f16bc4",
   "metadata": {},
   "source": [
    "## CREATE DATASET\n",
    "use ConCate dataset to handle multiple datasets situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ce8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    path: List[Optional[str]],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    require_target: bool = True,\n",
    ") -> Optional[Dataset]:\n",
    "    if all(p is None for p in path):\n",
    "        return None\n",
    "    datasets = []\n",
    "    for p in path:\n",
    "        if p is not None:\n",
    "            dataset = SquadSeq2SeqDataset(\n",
    "                Path(p), tokenizer, max_source_len=MAX_SOURCE_LEN, max_target_len=MAX_TARGET_LEN, require_target=require_target\n",
    "            )\n",
    "            datasets.append(dataset)\n",
    "    print(f\"Built dataset with {sum(len(ds) for ds in datasets)} samples.\")\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n",
    "\n",
    "def build_dataloader(\n",
    "    source: Union[Optional[Dataset], Optional[str]],\n",
    "    batch_size: int = 4,\n",
    "    shuffle: bool = False,\n",
    "    num_workers: int = 8,\n",
    ") -> Optional[DataLoader]:\n",
    "    dataset = source\n",
    "    collator = QACollator # Don't forget to define QACollator in data_utils.py\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collator,\n",
    "        num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b937d6",
   "metadata": {},
   "source": [
    "## Main loop of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b1f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    device: torch.device,\n",
    "    optimizer: Optional[torch.optim.Optimizer],\n",
    "    scheduler: Optional[object],\n",
    "    pad_id: int,\n",
    "    max_grad_norm: float,\n",
    "    train: bool,\n",
    ") -> float:\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    iterator = tqdm(dataloader, desc=\"train\" if train else \"eval\", leave=False)\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # 注意：這裡的 src 和 tgt 都是 1D 的 Packed Tensor\n",
    "        src = batch[\"src\"].to(device) # (Total_Src_Tokens,)\n",
    "        tgt = batch[\"tgt\"].to(device) # (Total_Tgt_Tokens,)\n",
    "        src_len = batch[\"src_len\"].to(device) # (B,)\n",
    "        tgt_len = batch[\"tgt_len\"].to(device) # (B,)\n",
    "\n",
    "        ############### YOUR CODE HERE (FLASH ATTENTION VERSION) ###############\n",
    "        \n",
    "        # 因為 tgt 是多個句子串接在一起的 (Packed)，我們不能直接用 [:, :-1] 切片\n",
    "        # 我們需要利用 cumsum 找出每個句子的邊界\n",
    "        \n",
    "        # 1. 計算累積長度來找出每個句子的邊界\n",
    "        cu_len = torch.cumsum(tgt_len, dim=0, dtype=torch.long)\n",
    "        \n",
    "        # 找出每個句子最後一個 Token (EOS) 的位置 -> 用於 Decoder Input (要移除)\n",
    "        end_indices = cu_len - 1\n",
    "        \n",
    "        # 找出每個句子第一個 Token (BOS) 的位置 -> 用於 Labels (要移除)\n",
    "        start_indices = cu_len - tgt_len\n",
    "        \n",
    "        # 2. 製作 Mask 並切片\n",
    "        total_tokens = tgt.size(0)\n",
    "        \n",
    "        # 製作 Decoder Input: 保留所有 token，但移除每個句子的最後一個 token (EOS)\n",
    "        mask_in = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_in[end_indices] = False\n",
    "        dec_in = tgt[mask_in]\n",
    "        \n",
    "        # 製作 Labels: 保留所有 token，但移除每個句子的第一個 token (BOS)\n",
    "        mask_label = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_label[start_indices] = False\n",
    "        labels = tgt[mask_label]\n",
    "        \n",
    "        # 調整長度 (每個句子都少了一個 token)\n",
    "        dec_len = tgt_len - 1\n",
    "\n",
    "        # 3. Forward Pass\n",
    "        logits = model(\n",
    "            src_input_ids=src,\n",
    "            trg_input_ids=dec_in,\n",
    "            src_seq_len=src_len,\n",
    "            trg_seq_len=dec_len\n",
    "        )\n",
    "\n",
    "        # 4. Compute Loss\n",
    "        # logits: (Total_Tokens, Vocab_Size), labels: (Total_Tokens,)\n",
    "        # 直接計算 CrossEntropy，不需要再 reshape\n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=pad_id)\n",
    "        \n",
    "        ######################################################\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        iterator.set_postfix(loss=total_loss / max(1, steps))\n",
    "        \n",
    "    return total_loss / max(1, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c1705",
   "metadata": {},
   "source": [
    "## Checkpoints management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f457f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    path: Path,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[object],\n",
    "    path: Path,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    if scheduler is not None and hasattr(scheduler, \"state_dict\"):\n",
    "        state[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "    torch.save(state, path)\n",
    "\n",
    "# (選用) 如果你要繼續訓練，建議改用這個版本來同時載入 Optimizer\n",
    "def load_checkpoint_for_training(\n",
    "    model, optimizer, scheduler, path, device\n",
    "):\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and \"scheduler_state_dict\" in state:\n",
    "        scheduler.load_state_dict(state[\"scheduler_state_dict\"])\n",
    "    print(f\"已載入 Epoch {state['epoch']} 的完整訓練狀態\")\n",
    "    return state[\"epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079efad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5cccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters and arguments ###\n",
    "lr = 8e-5\n",
    "weight_decay = 0.001\n",
    "warmup_steps = 2000\n",
    "epochs = TRAIN_EPOCHS\n",
    "max_grad_norm = 1.0\n",
    "batch_size = TRAIN_BATCH_SIZE\n",
    "num_workers = NUM_WORKERS\n",
    "#####################################\n",
    "set_seed(GLOBAL_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA is required to run this code.\")\n",
    "\n",
    "# Check if flash attention is available\n",
    "try:\n",
    "    import flash_attn  # noqa: F401\n",
    "except ImportError:\n",
    "    raise ImportError(\"flash_attn is required to run this code.\")\n",
    "\n",
    "model = Seq2SeqModelWithFlashAttn(\n",
    "    transformer_model_path=\"answerdotai/ModernBERT-base\",\n",
    "    freeze_encoder=True,\n",
    ").to(device)\n",
    "tokenizer = model.tokenizer\n",
    "checkpoint_path = CHECKPOINT_PATH\n",
    "best_checkpoint_path = BEST_CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c39fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset with 44229 samples.\n",
      "Built dataset with 5030 samples.\n"
     ]
    }
   ],
   "source": [
    "train_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_train.jsonl\", \"dataset/samsun/train.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "train_loader = build_dataloader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "val_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_val.jsonl\", \"dataset/samsun/validation.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "valid_loader = build_dataloader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "total_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=min(warmup_steps, total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529b628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 確保你已經定義了 model 和 device，並引入了 Path\n",
    "# from pathlib import Path\n",
    "# latest_ckpt_path = Path(\"checkpoints/latest.pt\")\n",
    "# if latest_ckpt_path.exists():\n",
    "#     start_epoch = load_checkpoint_for_training(model, optimizer, scheduler, latest_ckpt_path, device)\n",
    "#     print(f\"成功載入模型權重：{latest_ckpt_path}\")\n",
    "# else:\n",
    "#     print(f\"找不到檔案：{latest_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - train loss: 11.1582 | val loss: 8.0469 | ppl: 3124.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  53%|█████▎    | 91/173 [00:22<00:20,  4.02it/s, loss=7.84]"
     ]
    }
   ],
   "source": [
    "best_val_ppl = float(\"inf\")\n",
    "for epoch in range(start_epoch + 1, epochs + 1):\n",
    "    train_loss = run_epoch(\n",
    "        train_loader,\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        tokenizer.pad_token_id,\n",
    "        max_grad_norm,\n",
    "        train=True,\n",
    "    )\n",
    "    msg = f\"Epoch {epoch}/{epochs} - train loss: {train_loss:.4f}\"\n",
    "    current_val_ppl = None\n",
    "    with torch.no_grad():\n",
    "        val_loss = run_epoch(\n",
    "            valid_loader,\n",
    "            model,\n",
    "            device,\n",
    "            optimizer=None,\n",
    "            scheduler=None,\n",
    "            pad_id=tokenizer.pad_token_id,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            train=False,\n",
    "        )\n",
    "    perplexity = math.exp(min(20, val_loss))\n",
    "    current_val_ppl = perplexity\n",
    "    msg += f\" | val loss: {val_loss:.4f} | ppl: {perplexity:.2f}\"\n",
    "    print(msg)\n",
    "    if checkpoint_path is not None:    \n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    if (\n",
    "        current_val_ppl is not None\n",
    "        and current_val_ppl < best_val_ppl\n",
    "        and best_checkpoint_path is not None\n",
    "    ):\n",
    "        best_val_ppl = current_val_ppl\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=best_checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbbba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 原始日誌字串 ---\n",
    "log_string = \"\"\"\n",
    "Epoch 1/30 - train loss: 8.7638 | val loss: 6.9381 | ppl: 1030.82\n",
    "                                                                   \n",
    "Epoch 2/30 - train loss: 6.7033 | val loss: 6.2065 | ppl: 495.96\n",
    "                                                                   \n",
    "Epoch 3/30 - train loss: 6.0818 | val loss: 5.7010 | ppl: 299.16\n",
    "                                                                   \n",
    "Epoch 4/30 - train loss: 5.7039 | val loss: 5.4553 | ppl: 233.99\n",
    "                                                                   \n",
    "Epoch 5/30 - train loss: 5.4383 | val loss: 5.2457 | ppl: 189.75\n",
    "                                                                   \n",
    "Epoch 6/30 - train loss: 5.1855 | val loss: 5.0729 | ppl: 159.64\n",
    "                                                                   \n",
    "Epoch 7/30 - train loss: 4.9982 | val loss: 4.9571 | ppl: 142.18\n",
    "                                                                   \n",
    "Epoch 8/30 - train loss: 4.8601 | val loss: 4.8750 | ppl: 130.97\n",
    "                                                                   \n",
    "Epoch 9/30 - train loss: 4.7542 | val loss: 4.7981 | ppl: 121.28\n",
    "                                                                   \n",
    "Epoch 10/30 - train loss: 4.6684 | val loss: 4.7381 | ppl: 114.21\n",
    "                                                                   \n",
    "Epoch 11/30 - train loss: 4.6021 | val loss: 4.6900 | ppl: 108.85\n",
    "                                                                   \n",
    "Epoch 12/30 - train loss: 4.5514 | val loss: 4.6581 | ppl: 105.43\n",
    "                                                                   \n",
    "Epoch 13/30 - train loss: 4.5083 | val loss: 4.6342 | ppl: 102.94\n",
    "                                                                   \n",
    "Epoch 14/30 - train loss: 4.4744 | val loss: 4.6109 | ppl: 100.58\n",
    "                                                                   \n",
    "Epoch 15/30 - train loss: 4.4458 | val loss: 4.6017 | ppl: 99.66\n",
    "                                                                   \n",
    "Epoch 16/30 - train loss: 4.4242 | val loss: 4.5864 | ppl: 98.14\n",
    "                                                                   \n",
    "Epoch 17/30 - train loss: 4.4043 | val loss: 4.5692 | ppl: 96.47\n",
    "                                                                   \n",
    "Epoch 18/30 - train loss: 4.3873 | val loss: 4.5582 | ppl: 95.41\n",
    "                                                                   \n",
    "Epoch 19/30 - train loss: 4.3742 | val loss: 4.5545 | ppl: 95.06\n",
    "                                                                   \n",
    "Epoch 20/30 - train loss: 4.3646 | val loss: 4.5447 | ppl: 94.14\n",
    "                                                                   \n",
    "Epoch 21/30 - train loss: 4.3573 | val loss: 4.5447 | ppl: 94.14\n",
    "                                                                   \n",
    "Epoch 22/30 - train loss: 4.3493 | val loss: 4.5392 | ppl: 93.62\n",
    "                                                                   \n",
    "Epoch 23/30 - train loss: 4.3438 | val loss: 4.5377 | ppl: 93.47\n",
    "                                                                   \n",
    "Epoch 24/30 - train loss: 4.3403 | val loss: 4.5368 | ppl: 93.39\n",
    "                                                                   \n",
    "Epoch 25/30 - train loss: 4.3362 | val loss: 4.5325 | ppl: 92.99\n",
    "                                                                   \n",
    "Epoch 26/30 - train loss: 4.3346 | val loss: 4.5294 | ppl: 92.70\n",
    "                                                                   \n",
    "Epoch 27/30 - train loss: 4.3319 | val loss: 4.5294 | ppl: 92.70\n",
    "                                                                   \n",
    "Epoch 28/30 - train loss: 4.3311 | val loss: 4.5279 | ppl: 92.56\n",
    "                                                                   \n",
    "Epoch 29/30 - train loss: 4.3315 | val loss: 4.5273 | ppl: 92.51\n",
    "                                                                   \n",
    "Epoch 30/30 - train loss: 4.3299 | val loss: 4.5276 | ppl: 92.53\n",
    "\"\"\"\n",
    "\n",
    "# --- 數據提取與處理 ---\n",
    "epochs = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "ppl_values = []\n",
    "\n",
    "# 正則表達式捕捉 Epoch 數字和三個 metrics\n",
    "# Group 1: Epoch 數字\n",
    "# Group 2: train loss\n",
    "# Group 3: val loss\n",
    "# Group 4: ppl\n",
    "regex_pattern = r'Epoch (\\d+)/\\d+ - train loss: ([\\d.]+)\\s*\\| val loss: ([\\d.]+)\\s*\\| ppl: ([\\d.]+)'\n",
    "\n",
    "for line in log_string.split('\\n'):\n",
    "    match = re.search(regex_pattern, line.strip())\n",
    "    if match:\n",
    "        try:\n",
    "            epochs.append(int(match.group(1)))\n",
    "            train_losses.append(float(match.group(2)))\n",
    "            val_losses.append(float(match.group(3)))\n",
    "            ppl_values.append(float(match.group(4)))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# 將數據轉換為 DataFrame 以便結構化\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': epochs,\n",
    "    'Train Loss': train_losses,\n",
    "    'Val Loss': val_losses,\n",
    "    'PPL': ppl_values\n",
    "})\n",
    "\n",
    "# --- 繪圖設定與執行 (使用雙 Y 軸) ---\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "fig.suptitle('Training Metrics Trend', fontsize=16)\n",
    "\n",
    "# 設定 X 軸\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_xticks(df['Epoch']) # 確保 X 軸標籤只顯示實際的 Epoch 數字\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "# --- Y1 軸 (Losses) ---\n",
    "color_loss = 'tab:blue'\n",
    "ax1.set_ylabel('Loss Value', color=color_loss)\n",
    "ax1.tick_params(axis='y', labelcolor=color_loss)\n",
    "\n",
    "# 繪製 Train Loss 和 Val Loss (無 marker)\n",
    "line1 = ax1.plot(df['Epoch'], df['Train Loss'], linestyle='-', color='blue', label='Train Loss')\n",
    "line2 = ax1.plot(df['Epoch'], df['Val Loss'], linestyle='-', color='darkblue', label='Val Loss')\n",
    "\n",
    "\n",
    "# --- Y2 軸 (PPL) ---\n",
    "# 創建第二個 Y 軸\n",
    "ax2 = ax1.twinx()  \n",
    "color_ppl = 'tab:red'\n",
    "ax2.set_ylabel('PPL (Perplexity)', color=color_ppl) \n",
    "ax2.tick_params(axis='y', labelcolor=color_ppl)\n",
    "\n",
    "# 繪製 PPL (無 marker)\n",
    "line3 = ax2.plot(df['Epoch'], df['PPL'], linestyle='-', color='red', label='PPL')\n",
    "\n",
    "\n",
    "# 統一圖例 (Legend)\n",
    "lines = line1 + line2 + line3\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96]) # 調整佈局以容納主標題\n",
    "plt.show()\n",
    "\n",
    "# 輸出結果 (依照您的要求移除中文)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acffb21",
   "metadata": {},
   "source": [
    "## Predict Result\n",
    "\n",
    "Predict the labesl based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. To kaggle. Click \"Submit Predictions\"\n",
    "2. Upload the result.csv\n",
    "3. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, PREDICT_CHECKPOINT, device)\n",
    "model.eval()\n",
    "test_set = build_dataset(\n",
    "    [TIFU_TEST_PATH, SAMSUN_TEST_PATH],\n",
    "    tokenizer=model.tokenizer,\n",
    "    require_target=False,\n",
    ")\n",
    "test_loader = build_dataloader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "predictions: List[Tuple[str, str]] = []\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(test_loader, desc=\"predict\", leave=False):\n",
    "        input_ids = sample[\"src\"].to(device)\n",
    "        src_lens = sample[\"src_len\"].to(device=device, dtype=torch.int32)\n",
    "        ids = sample[\"id\"] #list of ids\n",
    "        summaries = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            src_seq_len=src_lens,\n",
    "            generation_limit=MAX_GENERATION_LEN,\n",
    "            sampling=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predictions.extend(zip(ids, summaries))\n",
    "output_path = PREDICTION_OUTPUT\n",
    "write_predictions_csv(output_path, predictions)\n",
    "print(f\"Wrote {len(predictions)} predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bb36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/user/.config/kaggle/kaggle.json'\n",
      "100%|███████████████████████████████████████| 0.99M/0.99M [00:01<00:00, 597kB/s]\n",
      "Successfully submitted to Lab6 Text Summarization with Seq2Seq Model(639401)"
     ]
    }
   ],
   "source": [
    "Message = (\"\"\n",
    "    +f\"TRAIN_EPOCHS = {TRAIN_EPOCHS}\\n\"\n",
    "    +f\"TRAIN_BATCH_SIZE = {TRAIN_BATCH_SIZE}\\n\"\n",
    "    +f\"LERANING_RATE = {lr}\\n\"\n",
    "    # +\"citerion1=nn.CrossEntropyLoss()\\n\"\n",
    ")\n",
    "!kaggle competitions submit -c lab-6-training-a-seq-2-seq-model-on-s-qu-ad-639401 -f result.csv -m \"{Message}\"    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
