{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b10c90",
   "metadata": {},
   "source": [
    "# 2025 DL Lab6: Text Summarization with Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5a614",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm 邱照元, 314834001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1d9c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assignment involves implementing a hybrid sequence-to-sequence model to perform text summarization on the SAMSum and Reddit TIFU datasets.\n",
    "\n",
    "The model architecture is composed of two main parts:\n",
    "A pre-trained model utilized as the encoder.\n",
    "A new decoder which must be implemented from scratch.\n",
    "\n",
    "The objective is to fine-tune the existing encoder while training the custom decoder from the beginning, enabling the complete model to generate accurate and concise summaries. Performance is measured using the standard summarization metric: ROUGE-L Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76750bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Lab6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu129\n",
      "Python Major.Minor: 3.12\n",
      "transformers version: 4.57.3\n",
      "ABI: True\n",
      "Flash Attention version: 2.8.3\n",
      "Is CUDA available: True\n",
      "CUDA Version: 12.9\n",
      "Device Name: NVIDIA GeForce RTX 5090\n",
      "Flash Attention calculation successful.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import flash_attn\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Python Major.Minor: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"ABI: {torch._C._GLIBCXX_USE_CXX11_ABI}\")\n",
    "print(f\"Flash Attention version: {flash_attn.__version__}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 簡單測試 (若無報錯即成功)\n",
    "q = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 1, 32, 64, device='cuda', dtype=torch.float16)\n",
    "from flash_attn import flash_attn_func\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Flash Attention calculation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce78d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "from data_utils import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformer.Const import *\n",
    "from transformer.Models import Seq2SeqModelWithFlashAttn\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODE = \"train\"  # set to \"predict\" for inference\n",
    "CHECKPOINT_PATH = Path(\"checkpoints/latest.pt\")\n",
    "BEST_CHECKPOINT_PATH = Path(\"checkpoints/best.pt\")\n",
    "PREDICT_CHECKPOINT = Path(\"checkpoints/best.pt\")\n",
    "TIFU_TEST_PATH = Path(\"dataset/tifu/tifu_test.jsonl\")\n",
    "SAMSUN_TEST_PATH = Path(\"dataset/samsun/test.csv\")\n",
    "PREDICTION_OUTPUT = Path(\"result.csv\")\n",
    "MAX_TARGET_LEN = 512\n",
    "MAX_GENERATION_LEN = MAX_TARGET_LEN\n",
    "TRAIN_EPOCHS = 30\n",
    "TRAIN_BATCH_SIZE = 100\n",
    "GLOBAL_SEED = 42\n",
    "NUM_WORKERS = 4\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d696f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f16bc4",
   "metadata": {},
   "source": [
    "## CREATE DATASET\n",
    "use ConCate dataset to handle multiple datasets situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ce8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    path: List[Optional[str]],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    require_target: bool = True,\n",
    ") -> Optional[Dataset]:\n",
    "    if all(p is None for p in path):\n",
    "        return None\n",
    "    datasets = []\n",
    "    for p in path:\n",
    "        if p is not None:\n",
    "            dataset = SquadSeq2SeqDataset(\n",
    "                Path(p), tokenizer, max_source_len=MAX_SOURCE_LEN, max_target_len=MAX_TARGET_LEN, require_target=require_target\n",
    "            )\n",
    "            datasets.append(dataset)\n",
    "    print(f\"Built dataset with {sum(len(ds) for ds in datasets)} samples.\")\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n",
    "\n",
    "def build_dataloader(\n",
    "    source: Union[Optional[Dataset], Optional[str]],\n",
    "    batch_size: int = 4,\n",
    "    shuffle: bool = False,\n",
    "    num_workers: int = 8,\n",
    ") -> Optional[DataLoader]:\n",
    "    dataset = source\n",
    "    collator = QACollator # Don't forget to define QACollator in data_utils.py\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collator,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b937d6",
   "metadata": {},
   "source": [
    "## Main loop of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b1f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    device: torch.device,\n",
    "    optimizer: Optional[torch.optim.Optimizer],\n",
    "    scheduler: Optional[object],\n",
    "    pad_id: int,\n",
    "    max_grad_norm: float,\n",
    "    train: bool,\n",
    ") -> float:\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    iterator = tqdm(dataloader, desc=\"train\" if train else \"eval\", leave=False)\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # 注意：這裡的 src 和 tgt 都是 1D 的 Packed Tensor\n",
    "        src = batch[\"src\"].to(device) # (Total_Src_Tokens,)\n",
    "        tgt = batch[\"tgt\"].to(device) # (Total_Tgt_Tokens,)\n",
    "        src_len = batch[\"src_len\"].to(device) # (B,)\n",
    "        tgt_len = batch[\"tgt_len\"].to(device) # (B,)\n",
    "\n",
    "        ############### YOUR CODE HERE (FLASH ATTENTION VERSION) ###############\n",
    "        \n",
    "        # 因為 tgt 是多個句子串接在一起的 (Packed)，我們不能直接用 [:, :-1] 切片\n",
    "        # 我們需要利用 cumsum 找出每個句子的邊界\n",
    "        \n",
    "        # 1. 計算累積長度來找出每個句子的邊界\n",
    "        cu_len = torch.cumsum(tgt_len, dim=0, dtype=torch.long)\n",
    "        \n",
    "        # 找出每個句子最後一個 Token (EOS) 的位置 -> 用於 Decoder Input (要移除)\n",
    "        end_indices = cu_len - 1\n",
    "        \n",
    "        # 找出每個句子第一個 Token (BOS) 的位置 -> 用於 Labels (要移除)\n",
    "        start_indices = cu_len - tgt_len\n",
    "        \n",
    "        # 2. 製作 Mask 並切片\n",
    "        total_tokens = tgt.size(0)\n",
    "        \n",
    "        # 製作 Decoder Input: 保留所有 token，但移除每個句子的最後一個 token (EOS)\n",
    "        mask_in = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_in[end_indices] = False\n",
    "        dec_in = tgt[mask_in]\n",
    "        \n",
    "        # 製作 Labels: 保留所有 token，但移除每個句子的第一個 token (BOS)\n",
    "        mask_label = torch.ones(total_tokens, dtype=torch.bool, device=device)\n",
    "        mask_label[start_indices] = False\n",
    "        labels = tgt[mask_label]\n",
    "        \n",
    "        # 調整長度 (每個句子都少了一個 token)\n",
    "        dec_len = tgt_len - 1\n",
    "\n",
    "        # 3. Forward Pass\n",
    "        logits = model(\n",
    "            src_input_ids=src,\n",
    "            trg_input_ids=dec_in,\n",
    "            src_seq_len=src_len,\n",
    "            trg_seq_len=dec_len\n",
    "        )\n",
    "\n",
    "        # 4. Compute Loss\n",
    "        # logits: (Total_Tokens, Vocab_Size), labels: (Total_Tokens,)\n",
    "        # 直接計算 CrossEntropy，不需要再 reshape\n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=pad_id)\n",
    "        \n",
    "        ######################################################\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        iterator.set_postfix(loss=total_loss / max(1, steps))\n",
    "        \n",
    "    return total_loss / max(1, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c1705",
   "metadata": {},
   "source": [
    "## Checkpoints management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f457f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    path: Path,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: Seq2SeqModelWithFlashAttn,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[object],\n",
    "    path: Path,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    if scheduler is not None and hasattr(scheduler, \"state_dict\"):\n",
    "        state[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "    torch.save(state, path)\n",
    "\n",
    "# (選用) 如果你要繼續訓練，建議改用這個版本來同時載入 Optimizer\n",
    "def load_checkpoint_for_training(\n",
    "    model, optimizer, scheduler, path, device\n",
    "):\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and \"scheduler_state_dict\" in state:\n",
    "        scheduler.load_state_dict(state[\"scheduler_state_dict\"])\n",
    "    print(f\"已載入 Epoch {state['epoch']} 的完整訓練狀態\")\n",
    "    return state[\"epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079efad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5cccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters and arguments ###\n",
    "lr = 1e-4\n",
    "weight_decay = 0.001\n",
    "warmup_steps = 2000\n",
    "epochs = TRAIN_EPOCHS\n",
    "max_grad_norm = 1.0\n",
    "batch_size = TRAIN_BATCH_SIZE\n",
    "num_workers = NUM_WORKERS\n",
    "#####################################\n",
    "set_seed(GLOBAL_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA is required to run this code.\")\n",
    "\n",
    "# Check if flash attention is available\n",
    "try:\n",
    "    import flash_attn  # noqa: F401\n",
    "except ImportError:\n",
    "    raise ImportError(\"flash_attn is required to run this code.\")\n",
    "\n",
    "model = Seq2SeqModelWithFlashAttn(\n",
    "    transformer_model_path=\"answerdotai/ModernBERT-base\",\n",
    "    freeze_encoder=False,\n",
    ").to(device)\n",
    "tokenizer = model.tokenizer\n",
    "checkpoint_path = CHECKPOINT_PATH\n",
    "best_checkpoint_path = BEST_CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c39fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset with 44229 samples.\n",
      "Built dataset with 5030 samples.\n"
     ]
    }
   ],
   "source": [
    "train_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_train.jsonl\", \"dataset/samsun/train.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "train_loader = build_dataloader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "val_set = build_dataset(\n",
    "    [\"dataset/tifu/tifu_val.jsonl\", \"dataset/samsun/validation.csv\"],\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "valid_loader = build_dataloader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "total_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=min(warmup_steps, total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529b628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 確保你已經定義了 model 和 device，並引入了 Path\n",
    "# from pathlib import Path\n",
    "# latest_ckpt_path = Path(\"checkpoints/latest.pt\")\n",
    "# if latest_ckpt_path.exists():\n",
    "#     start_epoch = load_checkpoint_for_training(model, optimizer, scheduler, latest_ckpt_path, device)\n",
    "#     print(f\"成功載入模型權重：{latest_ckpt_path}\")\n",
    "# else:\n",
    "#     print(f\"找不到檔案：{latest_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9c08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m best_val_ppl = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch + \u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_loss = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m     current_val_ppl = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(dataloader, model, device, optimizer, scheduler, pad_id, max_grad_norm, train)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[32m     69\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     clip_grad_norm_(model.parameters(), max_grad_norm)\n\u001b[32m     72\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/.venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Lab6/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_val_ppl = float(\"inf\")\n",
    "for epoch in range(start_epoch + 1, epochs + 1):\n",
    "    train_loss = run_epoch(\n",
    "        train_loader,\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        tokenizer.pad_token_id,\n",
    "        max_grad_norm,\n",
    "        train=True,\n",
    "    )\n",
    "    msg = f\"Epoch {epoch}/{epochs} - train loss: {train_loss:.4f}\"\n",
    "    current_val_ppl = None\n",
    "    with torch.no_grad():\n",
    "        val_loss = run_epoch(\n",
    "            valid_loader,\n",
    "            model,\n",
    "            device,\n",
    "            optimizer=None,\n",
    "            scheduler=None,\n",
    "            pad_id=tokenizer.pad_token_id,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            train=False,\n",
    "        )\n",
    "    perplexity = math.exp(min(20, val_loss))\n",
    "    current_val_ppl = perplexity\n",
    "    msg += f\" | val loss: {val_loss:.4f} | ppl: {perplexity:.2f}\"\n",
    "    print(msg)\n",
    "    if checkpoint_path is not None:    \n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    if (\n",
    "        current_val_ppl is not None\n",
    "        and current_val_ppl < best_val_ppl\n",
    "        and best_checkpoint_path is not None\n",
    "    ):\n",
    "        best_val_ppl = current_val_ppl\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            path=best_checkpoint_path,\n",
    "            epoch=epoch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acffb21",
   "metadata": {},
   "source": [
    "## Predict Result\n",
    "\n",
    "Predict the labesl based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/69788476947b482b88e46c9565db190b).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. To kaggle. Click \"Submit Predictions\"\n",
    "2. Upload the result.csv\n",
    "3. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, PREDICT_CHECKPOINT, device)\n",
    "model.eval()\n",
    "test_set = build_dataset(\n",
    "    [TIFU_TEST_PATH, SAMSUN_TEST_PATH],\n",
    "    tokenizer=model.tokenizer,\n",
    "    require_target=False,\n",
    ")\n",
    "test_loader = build_dataloader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "predictions: List[Tuple[str, str]] = []\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(test_loader, desc=\"predict\", leave=False):\n",
    "        input_ids = sample[\"src\"].to(device)\n",
    "        src_lens = sample[\"src_len\"].to(device=device, dtype=torch.int32)\n",
    "        ids = sample[\"id\"] #list of ids\n",
    "        summaries = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            src_seq_len=src_lens,\n",
    "            generation_limit=MAX_GENERATION_LEN,\n",
    "            sampling=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predictions.extend(zip(ids, summaries))\n",
    "output_path = PREDICTION_OUTPUT\n",
    "write_predictions_csv(output_path, predictions)\n",
    "print(f\"Wrote {len(predictions)} predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c lab-6-training-a-seq-2-seq-model-on-s-qu-ad-639401 -f result.csv -m \"Message\"    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
